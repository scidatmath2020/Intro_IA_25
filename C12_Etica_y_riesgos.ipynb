{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddf0d1f",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f15cf5",
   "metadata": {},
   "source": [
    "# Ética, Riesgos y Gobernanza en Sistemas de Inteligencia Artificial\n",
    "\n",
    "La inteligencia artificial ha dejado de ser una tecnología emergente para convertirse en un pilar fundamental de nuestra sociedad digital. Sin embargo, su rápido avance plantea dilemas éticos complejos que requieren un análisis profundo. En este capítulo, exploraremos no solo los riesgos técnicos, sino también las implicaciones filosóficas, legales y sociales de los sistemas de IA:\n",
    "\n",
    "- ¿Puede una máquina tomar decisiones verdaderamente éticas?\n",
    "\n",
    "- ¿Dónde deberían estar los límites de la autonomía de los sistemas de IA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb74f8",
   "metadata": {},
   "source": [
    "## Introducción a Riesgos Adicionales y Ética en la IA\n",
    "\n",
    "Abordemos la importancia de considerar la ética y la gestión de riesgos en IA, pues ignorarlos puede llevar a sesgos, violaciones de privacidad y daños. \n",
    "\n",
    "Tomemos el caso de IBM Watson, que enfrentó críticas por no cumplir con lo prometido en sus recomendaciones para tratamientos contra el cáncer. Al no gestionar adecuadamente las expectativas y garantizar transparencia, la confianza en el sistema de IA se vio erosionada. Este ejemplo subraya dos principios clave en ética de IA:\n",
    "\n",
    "- **Gestión de expectativas:** Las promesas excesivas sin base científica dañan la credibilidad\n",
    "\n",
    "- **Transparencia algorítmica:** Los profesionales médicos necesitan entender cómo se generan las recomendaciones para confiar en ellas\n",
    "\n",
    "**¿Cómo podrían haberse implementado salvaguardas éticas desde el diseño inicial de este sistema?**\n",
    "\n",
    "A medida que la IA se vuelve cada vez más común, los gobiernos y organismos reguladores están desarrollando políticas y guías para garantizar un uso responsable. Por ejemplo, el Reglamento General de Protección de Datos (GDPR) de la Unión Europea tiene disposiciones específicas relacionadas con la IA, como el derecho a la explicación y el requisito de implementar la protección de datos desde el diseño.\n",
    "\n",
    "Los sistemas de IA a menudo dependen de grandes cantidades de datos personales para su entrenamiento y operación. Garantizar la privacidad y seguridad de estos datos es crucial para proteger los derechos individuales y mantener la confianza de los clientes.\n",
    "\n",
    "En 2018, una brecha de seguridad en Facebook permitió el acceso no autorizado a datos personales de millones de usuarios, que luego fueron utilizados por Cambridge Analytica para publicidad política dirigida. Este caso resalta la importancia de medidas sólidas de privacidad y seguridad de datos.\n",
    "\n",
    "Los sistemas de IA a veces pueden operar como “cajas negras”, lo que dificulta entender cómo llegan a sus decisiones. Garantizar transparencia y explicabilidad en los sistemas de IA es esencial para la rendición de cuentas y la confianza, especialmente cuando la IA se utiliza en procesos de toma de decisiones críticas.\n",
    "\n",
    "A medida que los sistemas de IA se vuelven más capaces, podrían desplazar trabajos humanos mientras crean nuevas oportunidades en otras áreas. Abordar el impacto potencial de la IA en la fuerza laboral es vital para garantizar una transición fluida y minimizar las consecuencias negativas para los trabajadores y la sociedad.\n",
    "\n",
    "Por ejemplo, el auge de los vehículos autónomos tiene el potencial de interrumpir las industrias de transporte y taxis, provocando desplazamiento laboral de conductores. Sin embargo, también podría crear nuevas oportunidades en áreas como el mantenimiento y la gestión de flotas de sistemas de IA.\n",
    "\n",
    "Las tecnologías de IA también pueden ser mal utilizadas o aplicadas de forma maliciosa, provocando daños potenciales o consecuencias no deseadas. Es crucial considerar el posible uso indebido de los sistemas de IA e implementar salvaguardas para minimizar estos riesgos.\n",
    "\n",
    "Un ejemplo es el desarrollo de los “deepfakes”, donde la IA se usa para crear videos o imágenes falsos pero convincentes, que potencialmente difunden desinformación o dañan reputaciones.\n",
    "\n",
    "A medida que los sistemas de IA se vuelven más autónomos, surgen preguntas sobre la responsabilidad legal por sus acciones. Determinar quién es responsable cuando un sistema de IA causa daño o comete un error es un reto constante que las organizaciones deben considerar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271241e",
   "metadata": {},
   "source": [
    "## Sesgo\n",
    "\n",
    "El sesgo en sistemas de IA puede originarse por múltiples razones, como datos de entrenamiento sesgados, características inapropiadas o algoritmos defectuosos. Existen varios tipos: el sampling bias ocurre cuando los datos no representan adecuadamente a la población objetivo; el label bias surge de etiquetas incorrectas en el conjunto de entrenamiento; el aggregation bias aparece al combinar datos de distintas fuentes sin considerar variaciones; y el confirmation bias se da cuando el sistema refuerza creencias preexistentes. Otros sesgos incluyen el de evaluación, el de automatización y el de reporte.\n",
    "\n",
    "El *sesgo implícito* hace referencia a prejuicios inconscientes que pueden influir en la toma de decisiones humanas y ser incorporados sin intención en la IA. Para evitar discriminación y garantizar beneficios equitativos, es esencial promover la equidad en la toma de decisiones con IA. Esto implica usar datos diversos y representativos, aplicar métricas de equidad (como paridad demográfica o calibración) y garantizar la transparencia algorítmica para detectar y corregir sesgos.\n",
    "\n",
    "Ejemplos reales muestran la importancia de estas prácticas. Un banco descubrió que su sistema de aprobación de préstamos estaba sesgado contra ciertos grupos minoritarios. Corrigió el problema ajustando el conjunto de entrenamiento, aplicando métricas de equidad y logrando así oportunidades más justas para todos los solicitantes. En salud, un estudio de 2019 evidenció que un sistema de predicción de necesidades médicas estaba sesgado contra pacientes minoritarios, lo que llevó a modificar datos y métricas para garantizar recomendaciones imparciales.\n",
    "\n",
    "Mitigar sesgos requiere acciones como auditorías regulares de los sistemas para detectar y corregir sesgos, capacitación de equipos en ética de IA y colaboración con actores diversos para identificar posibles problemas. Además, se recomienda involucrar a las partes interesadas desde distintos contextos para diseñar estrategias efectivas que minimicen los riesgos.\n",
    "\n",
    "Finalmente, muchas organizaciones están adoptando guías éticas para un desarrollo responsable y justo de la IA. Por ejemplo, los Principios de IA de Google buscan evitar la creación o refuerzo de sesgos injustos. Siguiendo pautas similares, las empresas pueden demostrar compromiso con la ética, fomentar una cultura de responsabilidad y asegurar que los sistemas de IA sean transparentes, equitativos y beneficiosos para todos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c2d8c",
   "metadata": {},
   "source": [
    "## Seguridad y privacidad de datos con IA\n",
    "\n",
    "La IA puede mejorar la privacidad y seguridad de los datos mediante detección de anomalías, enmascaramiento de información sensible y autenticación avanzada de usuarios. Estas técnicas ayudan a prevenir brechas de seguridad, proteger la privacidad y habilitar un análisis seguro de datos, siendo útiles en sectores como banca y salud. Sin embargo, la IA también presenta riesgos, como la recolección masiva de datos, el sesgo y discriminación, y la vigilancia excesiva, que pueden amenazar los derechos individuales si no se gestionan éticamente.\n",
    "\n",
    "Para proteger los datos de los usuarios y cumplir con regulaciones de privacidad, se recomienda la minimización de datos (recoger solo lo necesario), la anonimización, la agregación y la seudonimización. Ejemplos como la “differential privacy” de Apple y la herramienta Private Join and Compute de Google muestran cómo preservar la privacidad mientras se permite el análisis de datos. Estas prácticas se complementan con el enfoque de “privacy by design”, integrando consideraciones de privacidad desde el inicio del desarrollo del sistema.\n",
    "\n",
    "Otros proyectos, como OpenMinded de Andrew Trask, promueven el aprendizaje y análisis federado, permitiendo entrenar modelos sin necesidad de acceder directamente a los datos originales. Estas técnicas mantienen la información bajo control del propietario y reducen riesgos de exposición. Además, se recomienda realizar auditorías y evaluaciones de impacto de protección de datos (DPIAs) para identificar y mitigar riesgos potenciales asociados al procesamiento de datos.\n",
    "\n",
    "Es fundamental ser transparente sobre cómo el sistema de IA recopila, procesa y utiliza los datos, comunicando esta información mediante políticas de privacidad y formularios de consentimiento. También se debe otorgar a los usuarios control sobre su información, incluyendo el acceso, corrección y eliminación de datos, tal como estipula el GDPR con el “derecho al olvido”. Esto fortalece la confianza y el cumplimiento regulatorio.\n",
    "\n",
    "Finalmente, las organizaciones deben capacitar a su personal sobre la importancia de la privacidad y seguridad de datos, fomentar una cultura de conciencia en toda la organización y mantenerse informadas sobre los avances y regulaciones en la materia. Participar en conferencias, asociaciones y grupos de trabajo ayuda a compartir mejores prácticas y desarrollar enfoques sólidos para la protección de datos en IA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
