{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d464af22",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf5d68",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad\n",
    "\n",
    "Es el proceso de transformar un conjunto de datos de alta dimensión (con muchas variables o características) a uno de menor dimensión conservando la mayor cantidad posible de información relevante. Se usa para:\n",
    "\n",
    "- Visualización (reducir a 2D o 3D),\n",
    "\n",
    "- Eliminación de ruido o redundancia,\n",
    "\n",
    "- Mejora en rendimiento de algoritmos,\n",
    "\n",
    "- Prevención de sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0c4ad",
   "metadata": {},
   "source": [
    "# Descomposición en Valores Singulares (SVD)\n",
    "\n",
    "Imagina que eres un escultor y tienes frente a ti un bloque de mármol. Primero, lo giras cuidadosamente para que quede perfectamente alineado con tu espacio de trabajo. Luego comienzas a esculpir, pero no con la misma intensidad en todas las direcciones; algunas partes las trabajas más profundamente que otras, como si aplicaras un escalado selectivo. Finalmente, una vez terminada la escultura, la giras para colocarla en su posición final, lista para exhibirse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e7b08",
   "metadata": {},
   "source": [
    "## ¿Qué es la SVD?\n",
    "Para una matriz $A \\in \\mathbb{R}^{m \\times n}$, la descomposición en valores singulares (SVD) es:\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "donde:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ y $V \\in \\mathbb{R}^{n \\times n}$ son matrices ortogonales,\n",
    "- $\\Sigma$ es una matriz diagonal (o rectangular con ceros) con valores no negativos en la diagonal (los valores singulares)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c7c0a",
   "metadata": {},
   "source": [
    "En términos de nuestro escultor:\n",
    "1. **Giras** el bloque para alinearlo (eso hace $V^T$),\n",
    "2. **Esculpes** con diferentes profundidades (eso hace $\\Sigma$),\n",
    "3. **Colocas** el resultado en su orientación final (eso hace $U$).\n",
    "\n",
    "Así actúa el SVD: cambio de base → escalado → nueva orientación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924b92b",
   "metadata": {},
   "source": [
    "## Geometría de la transformación\n",
    "Aplicar $A$ a un vector es como:\n",
    "- cambiar de base con $V^T$, \n",
    "- escalar con $\\Sigma$,\n",
    "- rotar con $U$.\n",
    "\n",
    "Un círculo se transforma en una elipse cuyos ejes principales están dados por las columnas de $U$ y alargadas según los valores singulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c95e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A= [[3 1]\n",
      " [0 2]]\n",
      "U =\n",
      " [[ 0.95709203 -0.28978415]\n",
      " [ 0.28978415  0.95709203]]\n",
      "\n",
      "Valores singulares = [3.25661654 1.84240298]\n",
      "\n",
      "V^T =\n",
      " [[ 0.8816746   0.47185793]\n",
      " [-0.47185793  0.8816746 ]]\n",
      "Sigma= [[3.25661654 0.        ]\n",
      " [0.         1.84240298]]\n",
      "\n",
      "A reconstruida =\n",
      " [[3.00000000e+00 1.00000000e+00]\n",
      " [1.11022302e-16 2.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo numérico de SVD\n",
    "import numpy as np\n",
    "A = np.array([[3, 1], [0, 2]])\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "\n",
    "print('A=',A)\n",
    "print('U =\\n', U)\n",
    "print('\\nValores singulares =', S)\n",
    "print('\\nV^T =\\n', VT)\n",
    "\n",
    "# Reconstrucción de A\n",
    "Sigma = np.zeros_like(A, dtype=float)\n",
    "np.fill_diagonal(Sigma, S)\n",
    "print('Sigma=',Sigma)\n",
    "A_reconstructed = U @ Sigma @ VT\n",
    "print('\\nA reconstruida =\\n', A_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44c04e",
   "metadata": {},
   "source": [
    "## Aplicaciones\n",
    "| Aplicación | Descripción |\n",
    "|------------|-------------|\n",
    "| Compresión de imágenes | Se eliminan valores singulares pequeños |\n",
    "| PCA | PCA es SVD sobre la matriz centrada |\n",
    "| Resolución de sistemas | La pseudo-inversa usa SVD |\n",
    "| Filtrado de ruido | Se descartan componentes con bajo peso |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24eac98",
   "metadata": {},
   "source": [
    "## Aproximación de rango bajo (Teorema de Eckart–Young)\n",
    "\n",
    "Los valores singulares de la matriz $A$ son los números no negativos $\\sigma_i$ tales que $\\sigma_i=\\sqrt{\\lambda_i}$ donde $\\lambda_i$ es un valor propio de $A^TA$.\n",
    " \n",
    "> **Teorema de Eckart–Young**. Sea $A\\in\\mathbb{R}^{m\\times n}$ y con descomposición en valores singulares $$A=U\\Sigma V^T$$\n",
    "> \n",
    ">Los valores singulares $\\sigma_1,\\,\\sigma_2,...,\\sigma_r>0$ determinan cuánto contribuye cada componente (o dirección) a la acción de $A$.\n",
    ">\n",
    "> El Teorema de Eckart–Young dice que si deseas aproximar $A$ por otra matriz de rango reducido $A_r$, entonces la mejor elección (en el sentido de error cuadrático mínimo con norma de Frobenius) es:\n",
    ">$$A_r=U_r\\Sigma_rV_r^T$$ \n",
    ">donde solo conservas los primeros $r$ valores singulares y los vectores correspondientes. Es decir, estás \"tirando a la basura\" las componentes menos importantes (las más pequeñas).\n",
    "\n",
    "De esta manera, tu bloque de mármol aún será rotado y esculpido, pero no usarás todas tus herramientas. Decides concentrarte solo en los $r$ ejes más importantes de acción: los que definen la forma principal:\n",
    "\n",
    "- Los valores singulares grandes corresponden a herramientas que modifican mucho la forma.\n",
    "\n",
    "- Los valores pequeños afectan detalles finos que, en caso de apuro, puedes omitir.\n",
    "\n",
    "Es como si tallaras solo los rasgos principales del rostro y omitieras las arrugas o texturas del cabello. La escultura resultante sigue pareciendo un rostro, aunque con menos detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da655b7",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad con SVD\n",
    "\n",
    "Otra manera de interpretar el SVD es la siguiente: Sea $A\\in\\mathbb{R}^{m\\times n}$ con rango $\\rho$ y $\\sigma_i>0$ los valores singulares ordenados de mayor a menor, $u_i\\in\\mathbb{R}^m$ las columnas de $U$ (recordemos que son ortonormales) y $v_i\\in\\mathbb{R}^n$ las columnas de $V$ (recordemos que son ortonormales). Entonces \n",
    "\n",
    "$$A=\\sigma_1u_1v_1^T+\\sigma_2u_2v_2^T+...+\\sigma_\\rho u_\\rho v_\\rho^T$$\n",
    "\n",
    "Entonces la mejor aproximación a $A$ con rango $r$ es \n",
    "\n",
    "$$A_r=\\sigma_1u_1v_1^T+\\sigma_2u_2v_2^T+...+\\sigma_r u_r v_r^T$$\n",
    "\n",
    "Aunque $A_r$ vuelve a tener dimensiones $m\\times n$, en la expresión anterior está la magia: **$A_r$ es la suma de solo $r$ matrices de rango 1**, cada una de la forma $\\sigma_i u_i v_i^T$, y por lo tanto su **rango es exactamente $r$**.\n",
    "\n",
    "Esto implica que **todas las columnas de $A_r$** (y también sus filas) **viven en un subespacio de dimensión $r$**. En particular:\n",
    "\n",
    "- Las columnas de $A_r$ están contenidas en el subespacio generado por $\\{u_1, u_2, \\dots, u_r\\} \\subseteq \\mathbb{R}^m$,\n",
    "- Las filas de $A_r$ están contenidas en el subespacio generado por $\\{v_1, v_2, \\dots, v_r\\} \\subseteq \\mathbb{R}^n$.\n",
    "\n",
    "De esta manera, el SVD nos dice que **toda la acción de $A_r$** se reduce a una combinación óptima de solo $r$ direcciones fundamentales: $r$ verticales y $r$ horizontales.\n",
    "\n",
    "Este es el corazón del teorema de Eckart–Young: **de todas las matrices de rango $r$**, $A_r$ es la que **más se parece a $A$** en norma de Frobenius (o espectral). Y esa semejanza está construida, literalmente, **sumando capas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ac92c",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "- La SVD descompone una matriz en rotaciones y escalados.\n",
    "- Es una herramienta central en ciencia de datos, álgebra lineal y machine learning.\n",
    "- Permite compresión, reducción de ruido y análisis de estructuras internas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
